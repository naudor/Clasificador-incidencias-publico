{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leemos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo datos de entrenamiento\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "print(\"Leyendo datos de entrenamiento\");\n",
    "\n",
    "df = pd.read_csv(\"DataSets/TOTES_LES_INCIDENCIES_v4.csv\",  encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
	"# Limpiamos las filas que no estan completas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset = [\"caller_id\",\"short_description\",\"description\",\"assignment_group\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
	"# Como solo podemos comparar dos campos pero quiero utilizar 3, juntos los dos campos de datos a valorar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['concatenat'] = df['caller_id'] +'.\\r\\n'+ df['short_description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorizamos las categorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category_id'] = df['assignment_group'].factorize()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Función para sacar los accentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(s):\n",
    "    replacements = (\n",
    "        (\"á\", \"a\"),\n",
    "        (\"é\", \"e\"),\n",
    "        (\"í\", \"i\"),\n",
    "        (\"ó\", \"o\"),\n",
    "        (\"ú\", \"u\"),\n",
    "    )\n",
    "    for a, b in replacements:\n",
    "        s = s.replace(a, b).replace(a.upper(), b.upper())\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
	"# Limpiamos el texto y vectorizamos las descripciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizando descripciones\n"
     ]
    }
   ],
   "source": [
    "print(\"Vectorizando descripciones\")\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spacy.lang.es.stop_words import STOP_WORDS as es_stop\n",
    "from io import StringIO\n",
    "import string\n",
    "\n",
    "df['concatenat'] = df['concatenat'].apply(lambda fila: fila.lower())\n",
    "df['short_description'] = df['short_description'].apply(lambda fila: normalize(fila))\n",
    "\n",
    "final_stopwords_list = list(es_stop)\n",
    "final_stopwords_list.append('\\r\\n')\n",
    "final_stopwords_list.append(string.punctuation)\n",
    "final_stopwords_list.append(\"mg\")\n",
    "final_stopwords_list.append(\"comp\")\n",
    "final_stopwords_list.append(\"kp\")\n",
    "\n",
    "# convierte el texto en vectores con la frecuencia de las palabras\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words=final_stopwords_list,max_features=15000)\n",
    "features = tfidf.fit_transform(df.concatenat).toarray()\n",
    "labels = df.category_id\n",
    "\n",
    "category_id_df = df[['assignment_group', 'category_id']].drop_duplicates().sort_values('category_id')\n",
    "category_to_id = dict(category_id_df.values)\n",
    "id_to_category = dict(category_id_df[['category_id', 'assignment_group']].values)\n",
    "#features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo\n"
     ]
    }
   ],
   "source": [
    "print(\"Entrenando modelo\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "model = LinearSVC()\n",
    "\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df.index, test_size=0.2, random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      precision    recall  f1-score   support\n",
      "\n",
      "       S-NOW-ERP-INT       0.70      0.70      0.70       683\n",
      "             SNOW-BI       0.64      0.70      0.67      1211\n",
      "    SNOW-BIRCHMAN-BI       0.65      0.37      0.47       970\n",
      "   SNOW-BIRCHMAN-CRM       0.77      0.90      0.83      6391\n",
      "            SNOW-BPC       0.79      0.83      0.81      1108\n",
      "        SNOW-BPEOPLE       0.89      0.92      0.90      1896\n",
      "            SNOW-BPM       0.69      0.63      0.66       247\n",
      "      SNOW-COMERCIAL       0.81      0.87      0.84      1235\n",
      "      SNOW-CONTECNOW       0.00      0.00      0.00         5\n",
      "   SNOW-COORDINATORS       0.00      0.00      0.00         4\n",
      "         SNOW-CRM-IK       0.49      0.40      0.44        47\n",
      "       SNOW-DELOITTE       0.52      0.67      0.59        36\n",
      "     SNOW-DOCUMENTAL       0.62      0.72      0.67        18\n",
      "            SNOW-ERP       0.00      0.00      0.00         1\n",
      "         SNOW-ERP-IK       0.50      0.14      0.22        14\n",
      "      SNOW-ERP-ROLES       0.81      0.82      0.81       429\n",
      "        SNOW-FICO-CO       0.38      0.16      0.22       127\n",
      "        SNOW-FICO-FI       0.29      0.09      0.13        68\n",
      "    SNOW-FRONTOFFICE       0.72      0.69      0.71        80\n",
      "       SNOW-GOBIERNO       0.88      0.88      0.88       325\n",
      "       SNOW-HELPDESK       0.57      0.27      0.36        30\n",
      "     SNOW-INDUSTRIAL       0.62      0.64      0.63        25\n",
      "    SNOW-LABORATORIO       0.62      0.52      0.57       197\n",
      "      SNOW-LOGISTICA       0.63      0.23      0.34        82\n",
      "SNOW-OPERACIONES-BPM       0.49      0.20      0.28        91\n",
      "SNOW-OPERACIONES-EHS       0.29      0.04      0.07       350\n",
      " SNOW-OPERACIONES-MM       0.58      0.50      0.54        22\n",
      " SNOW-OPERACIONES-PP       0.52      0.46      0.49        74\n",
      " SNOW-OPERACIONES-QM       0.46      0.23      0.31       286\n",
      " SNOW-OPERACIONES-SD       0.34      0.16      0.22        87\n",
      " SNOW-OPERACIONES-WM       0.51      0.33      0.40        81\n",
      "         SNOW-ORTEMS       0.49      0.23      0.31       121\n",
      "SNOW-PROJECTMANAGERS       0.50      0.06      0.11        16\n",
      "         SNOW-QPLANT       0.68      0.56      0.61        41\n",
      "          SNOW-ROLES       0.38      0.23      0.29        22\n",
      "            SNOW-SAP       0.53      0.72      0.61        25\n",
      "    SNOW-SAP-PISHEET       0.33      0.09      0.14        11\n",
      "        SNOW-SAP-VIM       0.00      0.00      0.00         2\n",
      "  SNOW-SERIALIZACION       0.27      0.15      0.19        27\n",
      "        SNOW-SISGEST       0.00      0.00      0.00         4\n",
      "      SNOW-SLIMSTOCK       0.00      0.00      0.00         2\n",
      "            SNOW-SYC       1.00      0.25      0.40        12\n",
      "         SNOW-SYC-OT       0.00      0.00      0.00         2\n",
      "\n",
      "            accuracy                           0.76     16505\n",
      "           macro avg       0.49      0.38      0.40     16505\n",
      "        weighted avg       0.73      0.76      0.73     16505\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "unic_label_train = df.groupby(['assignment_group'])['assignment_group'].size()\n",
    "unic_label_train = unic_label_train[unic_label_train > 4].index.get_level_values(0).tolist()\n",
    "\n",
    "#print(metrics.classification_report(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred, \n",
    "                                  target_names=unic_label_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardando modelo\n",
      "Modelo guardado\n"
     ]
    }
   ],
   "source": [
    "print(\"Guardando modelo\")\n",
    "\n",
    "import pickle\n",
    "from tempfile import TemporaryFile\n",
    "\n",
    "with open(\"clasificacion_CAUS_modelo.pickle\", \"wb\") as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "with open(\"clasificacion_CAUS_vectorizador.pickle\", \"wb\") as file:\n",
    "    pickle.dump(tfidf, file)\n",
    "\n",
    "with open(\"clasificacion_CAUS_categorias.pickle\", \"wb\") as file:\n",
    "    pickle.dump(id_to_category, file)\n",
    "    \n",
    "print (\"Modelo guardado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
